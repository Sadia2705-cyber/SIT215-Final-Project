{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('c:/users/sadia/documents/python scripts/envs/gym/lib/site-packages')\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # the training action is any random action from within the environment action space\n",
    "    def action(self, env):\n",
    "        return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearner():\n",
    "    def __init__(self, parameters):\n",
    "        self.alpha = parameters['alpha']\n",
    "        self.gamma = parameters['gamma']\n",
    "        self.epsilon = parameters['epsilon']\n",
    "        super().__init__()\n",
    "\n",
    "    def initialize_frozen_lake_q_table(self, env):\n",
    "        self.q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "    def frozen_lake_training_action(self, env, observation):\n",
    "        self.previous_observation = observation\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return env.action_space.sample() # explore\n",
    "        else:\n",
    "            return np.argmax(self.q_table[observation]) # exploit\n",
    "\n",
    "    def frozen_lake_evaluation_action(self, observation):\n",
    "        return np.argmax(self.q_table[observation])\n",
    "\n",
    "    def frozen_lake_update(self, observation, action, reward):\n",
    "        # updates the previous observation qtable entry with the reward gained,\n",
    "        # uses the maximum/best future option always\n",
    "        old_value = self.q_table[self.previous_observation, action]\n",
    "        next_max = np.max(self.q_table[observation])\n",
    "        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_max)\n",
    "        self.q_table[self.previous_observation, action] = new_value  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDlearner():\n",
    "    def __init__(self, parameters):\n",
    "        self.alpha = parameters['alpha']\n",
    "        self.gamma = parameters['gamma']\n",
    "        self.epsilon = parameters['epsilon']\n",
    "        super().__init__()\n",
    "    def initialize_frozen_lake_q_policy(self, env):\n",
    "        self.q_policy = np.ones([env.observation_space.n, env.action_space.n])\n",
    "        self.obs_range = env.action_space.n\n",
    "\n",
    "    def frozen_lake_training_action(self, env, observation):\n",
    "        self.previous_observation = observation\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return env.action_space.sample() # explore\n",
    "        else:\n",
    "            # exploit using a probability weighted selection fom future states\n",
    "            # with the existing policy\n",
    "            next_actions = self.q_policy[observation]\n",
    "            next_actions_sum = sum(next_actions)\n",
    "            weighted_actions = [action / next_actions_sum for action in next_actions]\n",
    "            return np.random.choice(np.arange(self.obs_range), p=weighted_actions)\n",
    "\n",
    "    def frozen_lake_evaluation_action(self, observation):\n",
    "        next_actions = self.q_policy[observation]\n",
    "        next_actions_sum = sum(next_actions)\n",
    "        weighted_actions = [action / next_actions_sum for action in next_actions]\n",
    "        return np.random.choice(np.arange(self.obs_range), p=weighted_actions)\n",
    "\n",
    "    def frozen_lake_update(self, observation, action, reward):\n",
    "        # updates the policy with the reward gained, using a probability weighted\n",
    "        # selection fom future states with the existing policy\n",
    "        old_value = self.q_policy[self.previous_observation, action]\n",
    "        next_actions = self.q_policy[observation]\n",
    "        next_actions_sum = sum(next_actions)\n",
    "        weighted_actions = [action / next_actions_sum for action in next_actions]\n",
    "        next_action_score = np.random.choice(next_actions, p=weighted_actions)\n",
    "        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_action_score)\n",
    "        self.q_policy[self.previous_observation, action] = new_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Driver:\n",
    "    def __init__(self, params):\n",
    "        self.epochs = params['epochs']\n",
    "        self.env = params['env']\n",
    "        self.agent = params['agent']\n",
    "        self.training_rewards = []\n",
    "        self.evaluation_rewards = []\n",
    "    def run_frozen_lake_random(self):\n",
    "        training_action = lambda _observation: self.agent.action(self.env)\n",
    "        update = lambda _observation, _action, _reward: None\n",
    "        evaluation_action = training_action\n",
    "\n",
    "        self.run(training_action, update, evaluation_action)\n",
    "\n",
    "    def run_frozen_lake_qlearner(self):\n",
    "        self.agent.initialize_frozen_lake_q_table(self.env)\n",
    "\n",
    "        training_action = lambda observation: self.agent.frozen_lake_training_action(self.env, observation)\n",
    "        update = lambda observation, action, reward: self.agent.frozen_lake_update(observation, action, reward)\n",
    "        evaluation_action = lambda observation: self.agent.frozen_lake_evaluation_action(observation)\n",
    "\n",
    "        self.run(training_action, update, evaluation_action)\n",
    "\n",
    "    def run_frozen_lake_tdlearner(self):\n",
    "        self.agent.initialize_frozen_lake_q_policy(self.env)\n",
    "\n",
    "        training_action = lambda observation: self.agent.frozen_lake_training_action(self.env, observation)\n",
    "        update = lambda observation, action, reward: self.agent.frozen_lake_update(observation, action, reward)\n",
    "        evaluation_action = lambda observation: self.agent.frozen_lake_evaluation_action(observation)\n",
    "\n",
    "        self.run(training_action, update, evaluation_action)\n",
    "\n",
    "    # main engine: training and evaluation loop, plot then demonstrate\n",
    "    def run(self, training_action, update, evaluation_action):\n",
    "        for i in range(self.epochs):\n",
    "            if ((i + 1) % 1000 == 0):\n",
    "                print(\"progress: {}%\".format(100 * (i + 1) // self.epochs))\n",
    "            self.train_once(training_action, update)\n",
    "            self.evaluate_once(evaluation_action)\n",
    "\n",
    "        self.plot()\n",
    "        \n",
    "        try:\n",
    "            self.demonstrate(evaluation_action)\n",
    "        except NotImplementedError:\n",
    "            print(\"Cannot demonstrate: render method on env not implemented.\")\n",
    "\n",
    "    # a single instance of training of the agent in the environment\n",
    "    def train_once(self, training_action, update):\n",
    "        observation = self.env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            action = training_action(observation)\n",
    "            observation, reward, done, info = self.env.step(action)\n",
    "            episode_reward += reward\n",
    "            update(observation, action, reward)\n",
    "        self.training_rewards.append(episode_reward)\n",
    "\n",
    "    # a single instance of evaluation of the agent at it's current level of training\n",
    "    def evaluate_once(self, evaluation_action):\n",
    "        observation = self.env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            action = evaluation_action(observation)\n",
    "            observation, reward, done, info = self.env.step(action)\n",
    "            episode_reward += reward\n",
    "        self.evaluation_rewards.append(episode_reward)\n",
    "\n",
    "    # plot training and evaluation reward levels at each epoch\n",
    "    def plot(self):\n",
    "        plt.subplot('211')\n",
    "        plt.plot(self.training_rewards, linewidth=1)\n",
    "        plt.title('Training reward over time')\n",
    "        plt.ylabel('reward')\n",
    "        plt.xlabel('iterations')\n",
    "\n",
    "        plt.subplot('212')\n",
    "        plt.plot(self.evaluation_rewards, linewidth=1)\n",
    "        plt.title('Evaluation reward over time')\n",
    "        plt.ylabel('reward')\n",
    "        plt.xlabel('iterations')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # use the environments render method and print some additional info\n",
    "    # to the console. permit user input for repeated demonstrations in a loop\n",
    "    def demonstrate(self, evaluation_action):\n",
    "        user_input = 'Y'\n",
    "        while (user_input == 'Y'):\n",
    "            observation = self.env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            reward = 0\n",
    "            step = 0\n",
    "            while not done:\n",
    "                print(f\"Step: {step} | Cumulative Reward: {episode_reward}\")\n",
    "                step += 1\n",
    "                print(\"RENDERING...\")\n",
    "                self.env.render()\n",
    "                action = evaluation_action(observation)\n",
    "                print('observation: ', observation)\n",
    "                print('action: ', action)\n",
    "                print('reward: ', reward)\n",
    "                observation, reward, done, info = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "\n",
    "            user_input = input('Enter Y for another demo: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 100%\n"
     ]
    }
   ],
   "source": [
    "def frozen_lake_random():\n",
    "    agent = Random()\n",
    "    driver = Driver({\n",
    "        'epochs': 1000,\n",
    "        'env': gym.make('FrozenLake-v0'),\n",
    "        'agent': agent,\n",
    "    })\n",
    "    driver.run_frozen_lake_random()\n",
    "\n",
    "def frozen_lake_qlearner():\n",
    "    agent = Qlearner({\n",
    "        'alpha': 0.1,\n",
    "        'gamma': 0.6,\n",
    "        'epsilon': 0.3,\n",
    "    })\n",
    "    driver = Driver({\n",
    "        'epochs': 10000,\n",
    "        'env': gym.make('FrozenLake-v0'),\n",
    "        'agent': agent,\n",
    "    })\n",
    "    driver.run_frozen_lake_qlearner()\n",
    "\n",
    "def frozen_lake_tdlearner():\n",
    "    agent = TDlearner({\n",
    "        'alpha': 0.1,\n",
    "        'gamma': 0.6,\n",
    "        'epsilon': 0.3,\n",
    "    })\n",
    "    driver = Driver({\n",
    "        'epochs': 10000,\n",
    "        'env': gym.make('FrozenLake-v0'),\n",
    "        'agent': agent,\n",
    "    })\n",
    "    driver.run_frozen_lake_tdlearner()\n",
    "if __name__ == '__main__':\n",
    "    frozen_lake_random()\n",
    "    #frozen_lake_qlearner()\n",
    "    #frozen_lake_tdlearner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
