{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('c:/users/sadia/documents/python scripts/envs/gym/lib/site-packages')\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cartpole bucket hyperparameters\n",
    "CARTPOLE_POSITION_BUCKETS = 2\n",
    "CARTPOLE_POSITION_RANGE = (-2.0, 2.0)\n",
    "CARTPOLE_VELOCITY_BUCKETS = 8\n",
    "CARTPOLE_VELOCITY_RANGE = (-1.2, 1.2)\n",
    "CARTPOLE_THETA_BUCKETS = 16\n",
    "CARTPOLE_THETA_RANGE = (-0.08, 0.08)\n",
    "CARTPOLE_THETA_VELOCITY_BUCKETS = 6\n",
    "CARTPOLE_THETA_VELOCITY_RANGE = (-1.2, 1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # the training action is any random action from within the environment action space\n",
    "    def action(self, env):\n",
    "        return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearner():\n",
    "    def __init__(self, parameters):\n",
    "        self.alpha = parameters['alpha']\n",
    "        self.gamma = parameters['gamma']\n",
    "        self.epsilon = parameters['epsilon']\n",
    "        super().__init__()\n",
    "    def initialize_cartpole_q_table(self, env):\n",
    "            obs_space = CARTPOLE_POSITION_BUCKETS * CARTPOLE_VELOCITY_BUCKETS * CARTPOLE_THETA_BUCKETS * CARTPOLE_THETA_VELOCITY_BUCKETS\n",
    "            self.q_table = np.zeros([obs_space, env.action_space.n])\n",
    "\n",
    "            # establish weak priors to optimise training - if theta < 0, move left, if theta > 0 move right\n",
    "            for i in range(obs_space):\n",
    "                if (i % (CARTPOLE_THETA_BUCKETS * CARTPOLE_THETA_VELOCITY_BUCKETS) < (CARTPOLE_THETA_BUCKETS / 2)):\n",
    "                    self.q_table[i][0] = 0.1\n",
    "                elif (i % (CARTPOLE_THETA_BUCKETS * CARTPOLE_THETA_VELOCITY_BUCKETS) >= (CARTPOLE_THETA_BUCKETS / 2)):\n",
    "                    self.q_table[i][1] = 0.1\n",
    "\n",
    "    def cartpole_training_action(self, env, observation):\n",
    "        self.previous_observation = observation\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return env.action_space.sample() # explore\n",
    "        else:\n",
    "            return np.argmax(self.q_table[self._cartpole_obs_index(observation)]) # exploit\n",
    "\n",
    "    def cartpole_evaluation_action(self, observation):\n",
    "        return np.argmax(self.q_table[self._cartpole_obs_index(observation)])\n",
    "\n",
    "    def cartpole_update(self, observation, action, reward):\n",
    "        # updates the previous observation qtable entry with the reward gained,\n",
    "        # uses the maximum/best future option always\n",
    "        old_value = self.q_table[self._cartpole_obs_index(self.previous_observation), action]\n",
    "        next_max = np.max(self.q_table[self._cartpole_obs_index(observation)])\n",
    "        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_max)\n",
    "        self.q_table[self._cartpole_obs_index(self.previous_observation), action] = new_value\n",
    "\n",
    "    def _cartpole_obs_index(self, observation):\n",
    "        # because cartpole observations are continuous, we have to bucket them and\n",
    "        # calculate an index for the qtable\n",
    "        position, velocity, theta, theta_velocity = observation\n",
    "\n",
    "        bucketed_position = self._bucket(position, CARTPOLE_POSITION_BUCKETS, CARTPOLE_POSITION_RANGE)\n",
    "        bucketed_velocity = self._bucket(velocity, CARTPOLE_VELOCITY_BUCKETS, CARTPOLE_VELOCITY_RANGE)\n",
    "        bucketed_theta = self._bucket(theta, CARTPOLE_THETA_BUCKETS, CARTPOLE_THETA_RANGE)\n",
    "        bucketed_theta_velocity = self._bucket(theta_velocity, CARTPOLE_THETA_VELOCITY_BUCKETS, CARTPOLE_THETA_VELOCITY_RANGE)\n",
    "\n",
    "        position_index = (bucketed_position - 1) * CARTPOLE_VELOCITY_BUCKETS * CARTPOLE_THETA_BUCKETS * CARTPOLE_THETA_VELOCITY_BUCKETS\n",
    "        velocity_index = (bucketed_velocity - 1) * CARTPOLE_THETA_BUCKETS * CARTPOLE_THETA_VELOCITY_BUCKETS\n",
    "        theta_index = (bucketed_theta - 1) * CARTPOLE_THETA_VELOCITY_BUCKETS\n",
    "        theta_velocity_index = (bucketed_theta_velocity - 1)\n",
    "\n",
    "        index = position_index + velocity_index + theta_index + theta_velocity_index\n",
    "        return index\n",
    "\n",
    "    def _bucket(self, observation, num_buckets, obs_range):\n",
    "        # calculate bucket number\n",
    "        r_min = obs_range[0]\n",
    "        r_max = obs_range[1]\n",
    "        r_range = r_max - r_min\n",
    "        bucket_size = r_range / num_buckets\n",
    "        bucket = math.ceil((observation + r_range / 2) / bucket_size)\n",
    "\n",
    "        # bound\n",
    "        bucket = min(bucket, num_buckets)\n",
    "        bucket = max(bucket, 1)\n",
    "        return bucket\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Driver:\n",
    "    def __init__(self, params):\n",
    "        self.epochs = params['epochs']\n",
    "        self.env = params['env']\n",
    "        self.agent = params['agent']\n",
    "        self.training_rewards = []\n",
    "        self.evaluation_rewards = []\n",
    "    def run_cartpole_random(self):\n",
    "        training_action = lambda _observation: self.agent.action(self.env)\n",
    "        update = lambda _observation, _action, _reward: None\n",
    "        evaluation_action = training_action\n",
    "\n",
    "        self.run(training_action, update, evaluation_action)\n",
    "\n",
    "    def run_cartpole_qlearner(self):\n",
    "        self.agent.initialize_cartpole_q_table(self.env)\n",
    "\n",
    "        training_action = lambda observation: self.agent.cartpole_training_action(self.env, observation)\n",
    "        update = lambda observation, action, reward: self.agent.cartpole_update(observation, action, reward)\n",
    "        evaluation_action = lambda observation: self.agent.cartpole_evaluation_action(observation)\n",
    "\n",
    "        self.run(training_action, update, evaluation_action)\n",
    "\n",
    "    def run_cartpole_tdlearner(self):\n",
    "        self.agent.initialize_cartpole_q_policy(self.env)\n",
    "\n",
    "        training_action = lambda observation: self.agent.cartpole_training_action(self.env, observation)\n",
    "        update = lambda observation, action, reward: self.agent.cartpole_update(observation, action, reward)\n",
    "        evaluation_action = lambda observation: self.agent.cartpole_evaluation_action(observation)\n",
    "\n",
    "        self.run(training_action, update, evaluation_action)\n",
    "    # main engine: training and evaluation loop, plot then demonstrate\n",
    "    def run(self, training_action, update, evaluation_action):\n",
    "        for i in range(self.epochs):\n",
    "            if ((i + 1) % 1000 == 0):\n",
    "                print(\"progress: {}%\".format(100 * (i + 1) // self.epochs))\n",
    "            self.train_once(training_action, update)\n",
    "            self.evaluate_once(evaluation_action)\n",
    "\n",
    "        self.plot()\n",
    "        \n",
    "        try:\n",
    "            self.demonstrate(evaluation_action)\n",
    "        except NotImplementedError:\n",
    "            print(\"Cannot demonstrate: render method on env not implemented.\")\n",
    "\n",
    "    # a single instance of training of the agent in the environment\n",
    "    def train_once(self, training_action, update):\n",
    "        observation = self.env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            action = training_action(observation)\n",
    "            observation, reward, done, info = self.env.step(action)\n",
    "            episode_reward += reward\n",
    "            update(observation, action, reward)\n",
    "        self.training_rewards.append(episode_reward)\n",
    "\n",
    "    # a single instance of evaluation of the agent at it's current level of training\n",
    "    def evaluate_once(self, evaluation_action):\n",
    "        observation = self.env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            action = evaluation_action(observation)\n",
    "            observation, reward, done, info = self.env.step(action)\n",
    "            episode_reward += reward\n",
    "        self.evaluation_rewards.append(episode_reward)\n",
    "\n",
    "    # plot training and evaluation reward levels at each epoch\n",
    "    def plot(self):\n",
    "        plt.subplot('211')\n",
    "        plt.plot(self.training_rewards, linewidth=1)\n",
    "        plt.title('Training reward over time')\n",
    "        plt.ylabel('reward')\n",
    "        plt.xlabel('iterations')\n",
    "\n",
    "        plt.subplot('212')\n",
    "        plt.plot(self.evaluation_rewards, linewidth=1)\n",
    "        plt.title('Evaluation reward over time')\n",
    "        plt.ylabel('reward')\n",
    "        plt.xlabel('iterations')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # use the environments render method and print some additional info\n",
    "    # to the console. permit user input for repeated demonstrations in a loop\n",
    "    def demonstrate(self, evaluation_action):\n",
    "        user_input = 'Y'\n",
    "        while (user_input == 'Y'):\n",
    "            observation = self.env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            reward = 0\n",
    "            step = 0\n",
    "            while not done:\n",
    "                print(f\"Step: {step} | Cumulative Reward: {episode_reward}\")\n",
    "                step += 1\n",
    "                print(\"RENDERING...\")\n",
    "                self.env.render()\n",
    "                action = evaluation_action(observation)\n",
    "                print('observation: ', observation)\n",
    "                print('action: ', action)\n",
    "                print('reward: ', reward)\n",
    "                observation, reward, done, info = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "\n",
    "            user_input = input('Enter Y for another demo: ')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 2%\n",
      "progress: 4%\n",
      "progress: 6%\n",
      "progress: 8%\n",
      "progress: 10%\n",
      "progress: 12%\n",
      "progress: 14%\n",
      "progress: 16%\n",
      "progress: 18%\n",
      "progress: 20%\n",
      "progress: 22%\n",
      "progress: 24%\n",
      "progress: 26%\n",
      "progress: 28%\n",
      "progress: 30%\n",
      "progress: 32%\n",
      "progress: 34%\n",
      "progress: 36%\n",
      "progress: 38%\n",
      "progress: 40%\n",
      "progress: 42%\n",
      "progress: 44%\n",
      "progress: 46%\n",
      "progress: 48%\n",
      "progress: 50%\n",
      "progress: 52%\n",
      "progress: 54%\n",
      "progress: 56%\n",
      "progress: 58%\n",
      "progress: 60%\n",
      "progress: 62%\n",
      "progress: 64%\n",
      "progress: 66%\n",
      "progress: 68%\n",
      "progress: 70%\n",
      "progress: 72%\n",
      "progress: 74%\n",
      "progress: 76%\n",
      "progress: 78%\n",
      "progress: 80%\n",
      "progress: 82%\n",
      "progress: 84%\n",
      "progress: 86%\n",
      "progress: 88%\n",
      "progress: 90%\n",
      "progress: 92%\n",
      "progress: 94%\n",
      "progress: 96%\n",
      "progress: 98%\n",
      "progress: 100%\n"
     ]
    }
   ],
   "source": [
    "def cartpole_random():\n",
    "    agent = Random()\n",
    "    driver = Driver({\n",
    "        'epochs': 1000,\n",
    "        'env': gym.make('CartPole-v1'),\n",
    "        'agent': agent,\n",
    "    })\n",
    "    driver.run_cartpole_random()\n",
    "\n",
    "def cartpole_qlearner():\n",
    "    agent = Qlearner({\n",
    "        'alpha': 0.2,\n",
    "        'gamma': 0.5,\n",
    "        'epsilon': 0.1,\n",
    "    })\n",
    "    driver = Driver({\n",
    "        'epochs': 50000,\n",
    "        'env': gym.make('CartPole-v1'),\n",
    "        'agent': agent,\n",
    "    })\n",
    "    driver.run_cartpole_qlearner()\n",
    "\n",
    "def cartpole_tdlearner():\n",
    "    agent = TDlearner({\n",
    "        'alpha': 0.2,\n",
    "        'gamma': 0.5,\n",
    "        'epsilon': 0.1,\n",
    "    })\n",
    "    driver = Driver({\n",
    "        'epochs': 50000,\n",
    "        'env': gym.make('CartPole-v1'),\n",
    "        'agent': agent,\n",
    "    })\n",
    "    driver.run_cartpole_tdlearner()\n",
    "if __name__ == '__main__':\n",
    "    #cartpole_random()\n",
    "    cartpole_qlearner()\n",
    "    #cartpole_tdlearner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
